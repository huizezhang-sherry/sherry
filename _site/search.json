[
  {
    "objectID": "blogs/index.html",
    "href": "blogs/index.html",
    "title": "H. Sherry Zhang",
    "section": "",
    "text": "How CRAN packages are interconnected\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 18, 2022\n\n\nH. Sherry Zhang\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay 21, 2022\n\n\nH. Sherry Zhang\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blogs/2022-05-21-ggplot-sf/index.html",
    "href": "blogs/2022-05-21-ggplot-sf/index.html",
    "title": "How long do maps on ggplot facets take?",
    "section": "",
    "text": "This example comes from Chapter 7 of Paula Moraga’s book Geospatial Health Data: Modeling and Visualization with R-INLA and Shiny and I have simplified it for my demonstration. In essence, there are two datasets:\n\nMap data (ohio) with 88 Ohio counties in an sf object:\n\n\n\nSimple feature collection with 88 features and 1 field\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -84.8203 ymin: 38.40342 xmax: -80.5182 ymax: 42.32713\nGeodetic CRS:  NAD83\n# A tibble: 88 × 2\n   NAME                                                                 geometry\n   <chr>                                                           <POLYGON [°]>\n 1 Auglaize   ((-84.13476 40.65755, -84.13467 40.65755, -84.13405 40.65753, -84…\n 2 Crawford   ((-82.77258 40.99589, -82.77258 40.99588, -82.77168 40.99588, -82…\n 3 Montgomery ((-84.06231 39.8366, -84.06301 39.83665, -84.06501 39.83677, -84.…\n 4 Guernsey   ((-81.22986 40.06315, -81.22987 40.06308, -81.22992 40.06119, -81…\n 5 Clark      ((-83.83875 39.8233, -83.83889 39.82335, -83.83904 39.82339, -83.…\n 6 Gallia     ((-82.18737 38.72608, -82.18727 38.72558, -82.18707 38.72488, -82…\n 7 Fairfield  ((-82.82307 39.80773, -82.82307 39.8078, -82.82305 39.80801, -82.…\n 8 Darke      ((-84.43157 40.15801, -84.43148 40.15487, -84.43148 40.1542, -84.…\n 9 Monroe     ((-81.22569 39.57838, -81.24065 39.57883, -81.2413 39.57885, -81.…\n10 Portage    ((-81.3184 40.98861, -81.31892 40.98862, -81.31927 40.98862, -81.…\n# … with 78 more rows\n\n\n\nLung cancer data (sir) with standardized incidence ratios (SIRs) calculated for each county across 21 years (1968 - 1988):\n\n\n\n\n\n\n# A tibble: 1,848 × 3\n   county  year   SIR\n   <chr>  <dbl> <dbl>\n 1 Adams   1968 0.725\n 2 Adams   1969 0.588\n 3 Adams   1970 1.03 \n 4 Adams   1971 0.654\n 5 Adams   1972 1.05 \n 6 Adams   1973 0.693\n 7 Adams   1974 1.15 \n 8 Adams   1975 1.17 \n 9 Adams   1976 0.936\n10 Adams   1977 0.644\n# … with 1,838 more rows\n\n\nThe details on calculating SIR is not the focus of this post and Section 7.1 to 7.2 of Paula’s book has detailed all the steps you need if interested. Here I attach the script to generate these two data in case you would like to give it a spin:\n\n\nscript\n\n\n#remotes::install_github(\"Paula-Moraga/SpatialEpiApp\")\nlibrary(SpatialEpiApp)\nlibrary(SpatialEpi)\nlibrary(tidyverse)\nlibrary(sf)\n\n# ohio map data\nohio <- read_sf(system.file(\"SpatialEpiApp/data/Ohio/fe_2007_39_county/fe_2007_39_county.shp\", \n                           package = \"SpatialEpiApp\")) %>% \n  select(NAME, geometry)\n\n# sir case data\nraw <- read_csv(system.file(\"SpatialEpiApp/data/Ohio/dataohiocomplete.csv\", \n                    package = \"SpatialEpiApp\"))\n\ndt <- raw %>% arrange(county, year, gender, race) \nres <- dt %>% \n  group_by(NAME, year) %>% \n  summarise(Y = sum(y)) %>% \n  ungroup()\n\nn_strata <- 4\nE <- expected(population = dt$n, cases = dt$y, n.strata = n_strata)\nnyears <- length(unique(raw$year))\ncountiesE <- rep(unique(raw$NAME), each = nyears)\n\nncounties <- length(unique(raw$NAME))\nyearsE <- rep(unique(raw$year), time = ncounties)\n\nsir <- tibble(county = countiesE, year = yearsE, E = E) %>% \n  left_join(res, by = c(\"county\" = \"NAME\", \"year\")) %>% \n  mutate(SIR = Y/E) %>% \n  select(county, year, SIR)\n\n\nWhat we would like to do here is to show those SIR values of each county on the map across year. This would require us to join the two datasets, supply the combined data into ggplot, plot the underlying map, fill the county polygon with SIR, make facets with year, and lastly add a few tweaks on theme and the scale of fill. Let’s give this plot a name, say target:\n\ncombined <- ohio %>% \n  left_join(sir, by = c(\"NAME\" = \"county\"))\n\nWarning in sf_column %in% names(g): Each row in `x` is expected to match at most 1 row in `y`.\nℹ Row 1 of `x` matches multiple rows.\nℹ If multiple matches are expected, set `multiple = \"all\"` to silence this\n  warning.\n\ntarget <- combined %>% \n  ggplot() + \n  geom_sf(aes(fill = SIR)) +\n  facet_wrap(~year, dir = \"h\", ncol = 7) +\n  ggtitle(\"SIR\") + \n  theme_bw() +\n  theme(\n    axis.text.x = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks = element_blank()\n  ) +\n  scale_fill_gradient2(\n    midpoint = 1, low = \"blue\", mid = \"white\", high = \"red\"\n  )\n\ntarget\n\n\n\n\n\n\n\n\nEasy peasy.\nBut, have you thought about how long it would take to bring this plot to you?\n\n\nWarning in sf_column %in% names(g): Each row in `x` is expected to match at most 1 row in `y`.\nℹ Row 1 of `x` matches multiple rows.\nℹ If multiple matches are expected, set `multiple = \"all\"` to silence this\n  warning.\n\n\n\n\n\n\n\n\n\n\n\nLet me show you some components of this plot as benchmarks, here I have:\n\nP0: a single map object (left): 0.806 secs\nP1: a single year (1968) with SIR filled (mid): 0.902 secs, and\nP2: two years (1968 & 1969) with SIR filled in facets (right): 1.638 secs\n\n\n\n\n\n\n\n\n\n\nokay, now it is your time to make a guess:\n\n~1 or 2 seconds? Ideally if the same map is rendered in parallel across all the facets, the increment of time would be marginal.\n~16 seconds? The increment of rendering another facet from 2) to 3) is 0.736 (1.638-0.902) seconds. Projecting that into 20 more facets will give us: 0.902 + (1.638-0.902) * 20 = 16.358 seconds.\n30 seconds, 40 seconds, 1 minute? I don’t know.\n\n\nLet’s reveal the answer\nThere are different ways to check the execution time of a command and here we use ggplot2::benchplot(), which breaks down the creation time by constructing, building, rendering, and drawing:\n\n\nfunction (x) \n{\n    x <- enquo(x)\n    construct <- system.time(x <- eval_tidy(x))\n    if (!inherits(x, \"ggplot\")) {\n        abort(\"`x` must be a ggplot object\")\n    }\n    build <- system.time(data <- ggplot_build(x))\n    render <- system.time(grob <- ggplot_gtable(data))\n    draw <- system.time(grid.draw(grob))\n    times <- rbind(construct, build, render, draw)[, 1:3]\n    times <- rbind(times, colSums(times))\n    cbind(step = c(\"construct\", \"build\", \"render\", \"draw\", \"TOTAL\"), \n        mat_2_df(times))\n}\n<bytecode: 0x7fd993f25ff8>\n<environment: namespace:ggplot2>\n\n\nReady for the answer? Here you go:\n\nbenchplot(target)\n\n\n\n       step user.self sys.self elapsed\n1 construct     0.000    0.000   0.000\n2     build     1.289    0.050   1.336\n3    render     0.707    0.055   0.774\n4      draw    25.914    0.067  25.975\n5     TOTAL    27.910    0.172  28.085\n\n\nWOW, I do not expect it to take 28.085 seconds to get our plot!\n\n\nHow come it takes that long?\nWe can take a look at the time decomposition of our target plot along with the three benchmark plots. This would tell us at which stage our target plot takes long:\n\n\n\n\n\n\n\n\n\nHere p0 to p2 are the three benchmark plots and p21 is the target plot (since it has 21 facets). Notice the y-axis is the square root of elapsed time and the text on each bar is the actual elapsed time.\nBuilding and rendering times look fine, but our target plot is taking a considerable large amount of time in the drawing. Looking back into benchplot(), the drawing time is calculated as the time of grid.draw() drawing the grob:\n\ndraw <- system.time(grid.draw(grob))\n\nWe could also do an experiment to progressively add facets to see how the drawing time changes as there are more facets. Here I start with p1 containing only year 1968 and p2 containing year 1968 & 1969, and add one more year at a time till p21, which contains all the 21 years from 1968 to 1988. Here is the script I used to make the simulation:\n\ncombined <- ohio %>% left_join(sir, by = c(\"NAME\" = \"county\")) \n\nmake_plot <- function(data){\n  data %>% \n    ggplot() + \n    geom_sf(aes(fill = SIR)) + \n    theme_bw() +\n    facet_wrap(~year, dir = \"h\", ncol = 7) +\n    theme(\n      axis.text.x = element_blank(),\n      axis.text.y = element_blank(),\n      axis.ticks = element_blank(),\n    )  + \n    scale_fill_gradient2(\n      limits = c(0, range(combined$SIR)[2]),\n      midpoint = 1, low = \"blue\", mid = \"white\", high = \"red\", \n    )\n}\n\nbench_plot <- function(data){\n  p <- make_plot(data)\n  benchplot(p)\n}\n\ntime_all <- map_dfr(year, function(y){\n    dt <- combined %>% filter(year <= y)\n    dev.new()\n    out <- dt %>% bench_plot()\n    while (dev.cur()>1) dev.off()\n    return(out)\n}, .id = \"plot\")\n\nNote that the workhorse, out <- dt %>% bench_plot(), is wrapped in between dev.new() and while (dev.cur()>1) dev.off() so that a clean canvas is set up before each evaluation and closed after. Now we can plot the result and take a look:\n\n\n\n\n\n\n\n\n\nThe three dotted vertical lines are where a new row takes place in the facet. The dashed line connects p1 and p2 and shows how the elapsed time would be if a linear interpolation between p1 and p2 is followed. Unfortunately, This is not the case from our plot.\nLooking at these points, something wired is going on here: by the end of the first row, the increment from having six facets (p6) to seven facets (p7) is much larger than those in early plots (p1 to p5). However, the end of the second row tells us something else: the increment from having 13 facets (p13) to 14 facets (p14) is almost negligible. This is also the case at the end of the third row (p20 and p21)1.\n\n\nWarning in sf_column %in% names(g): Each row in `x` is expected to match at most 1 row in `y`.\nℹ Row 1 of `x` matches multiple rows.\nℹ If multiple matches are expected, set `multiple = \"all\"` to silence this\n  warning.\n\n\n\n\nBack to the initial problem\nWhile I can’t find the answer of why the drawing time with additional facets has such a pattern, what initially annoys me was it takes much longer than I expected to create the target plot. If we only want to cut the run time, there is always the trick of simplifying your map object. Applying rmapshaper:: ms_simplify() on ohio will keep 1% of the points in the polygons by default and it can instantly bring the rendering time of our target plot down to 2.283 seconds:\n\nohio2 <- ohio %>% rmapshaper::ms_simplify()\ncombined2 <- ohio2 %>% left_join(sir, by = c(\"NAME\" = \"county\"))\ntarget2 <- combined2 %>% make_plot()\nbenchplot(target2)\n\n\n\n       step user.self sys.self elapsed\n1 construct     0.000    0.000   0.000\n2     build     1.463    0.023   1.486\n3    render     0.517    0.006   0.523\n4      draw     0.272    0.003   0.274\n5     TOTAL     2.252    0.032   2.283\n\n\nAnd how does the plot look like after the simplification?\n\n\n\n\n\n\n\n\n\nAt least I can’t tell it from the original plot.\nHopefully, you’re as surprised as I do when first knowing how long it takes to render our facet map. We find that it is the drawing time that takes the majority of the time to create the plot and the time required to draw more facet is not a linear increase of the initial two facets. However, technically, we didn’t answer the question of why it takes that long to render the target plot or what grid.draw() is doing when plotting the facets. But even if we can’t answer it, a fast rendering is still available if we remember map simplification.\n\n\n\n\n\nFootnotes\n\n\nTo make a proper benchmark of time, ideally each plot (p1 - p21) should be evaluated repetitively to obtain a distribution of the elapsed time. I set up a script with 50 repetitions and let it run overnight, but what I got next morning was “RStudio quit unexpectedly”. I suspect there is something going on with opening and closing the graphic devices too many times…↩︎"
  },
  {
    "objectID": "blogs/2022-10-12-cran-ecosystem/index.html",
    "href": "blogs/2022-10-12-cran-ecosystem/index.html",
    "title": "Diving into dependen-“sea”",
    "section": "",
    "text": "Preparing dependency data\nThe utils package provides the function available.packages() to extract CRAN package information. The data includes information on the package name, version, dependency, and license:\n\n\nCode\n(raw <- utils::available.packages() %>% as_tibble())\n\n\n\n\n\n\n\n# A tibble: 18,692 × 17\n   Package     Version Priority Depends  Imports Linki…¹ Sugge…² Enhan…³ License\n   <chr>       <chr>   <chr>    <chr>    <chr>   <chr>   <chr>   <chr>   <chr>  \n 1 A3          1.0.0   <NA>     R (>= 2… <NA>    <NA>    random… <NA>    GPL (>…\n 2 AATtools    0.0.2   <NA>     R (>= 3… magrit… <NA>    <NA>    <NA>    GPL-3  \n 3 ABACUS      1.0.0   <NA>     R (>= 3… ggplot… <NA>    rmarkd… <NA>    GPL-3  \n 4 abbreviate  0.1     <NA>     <NA>     <NA>    <NA>    testth… <NA>    GPL-3  \n 5 abbyyR      0.5.5   <NA>     R (>= 3… httr, … <NA>    testth… <NA>    MIT + …\n 6 abc         2.2.1   <NA>     R (>= 2… <NA>    <NA>    <NA>    <NA>    GPL (>…\n 7 abc.data    1.0     <NA>     R (>= 2… <NA>    <NA>    <NA>    <NA>    GPL (>…\n 8 ABC.RAP     0.9.0   <NA>     R (>= 3… graphi… <NA>    knitr,… <NA>    GPL-3  \n 9 abcADM      1.0     <NA>     <NA>     Rcpp (… Rcpp, … <NA>    <NA>    GPL-3  \n10 ABCanalysis 1.2.1   <NA>     R (>= 2… plotrix <NA>    <NA>    <NA>    GPL-3  \n# … with 18,682 more rows, 8 more variables: License_is_FOSS <chr>,\n#   License_restricts_use <chr>, OS_type <chr>, Archs <chr>, MD5sum <chr>,\n#   NeedsCompilation <chr>, File <chr>, Repository <chr>, and abbreviated\n#   variable names ¹​LinkingTo, ²​Suggests, ³​Enhances\n\n\nFrom this, we can extract a table to map out the direct dependency every CRAN package has. In this post we will focus on the two strong dependencies: Depends and Imports:\n\n\nCode\nall_pkgs <- raw %>% \n  tidyr::separate_rows(Imports, sep = \",\") %>% \n  tidyr::separate_rows(Depends, sep = \",\") %>% \n  mutate(\n    across(c(Depends, Imports), ~gsub(\"\\\\(.*\\\\)\", \"\\\\1\", .x)),\n    across(c(Depends, Imports), str_trim)\n    )\n  # filter(!Depends %in% c(\"R\", \"\"), Imports != \"\", !is.na(Depends))\n\n(dep_lookup_tbl <- all_pkgs %>% \n  dplyr::select(Package, Depends, Imports) %>% \n  rename(downstream = Package) %>% \n  pivot_longer(Depends:Imports, names_to = \"type\", values_to = \"upstream\") %>% \n  distinct() %>% \n  filter(!upstream %in% c(\"R\", \"\")) %>% \n  filter(!is.na(upstream)) %>% \n  arrange(downstream))\n\n\n# A tibble: 96,765 × 3\n   downstream type    upstream  \n   <chr>      <chr>   <chr>     \n 1 A3         Depends xtable    \n 2 A3         Depends pbapply   \n 3 AATtools   Imports magrittr  \n 4 AATtools   Imports dplyr     \n 5 AATtools   Imports doParallel\n 6 AATtools   Imports foreach   \n 7 ABACUS     Imports ggplot2   \n 8 ABACUS     Imports shiny     \n 9 ABC.RAP    Imports graphics  \n10 ABC.RAP    Imports stats     \n# … with 96,755 more rows\n\n\nDependency is a transitive relation. This means a package also (indirectly) depends on all the dependencies of the package of it imports and so on. Changes from an package will propagate downwards through its dependency chain. With the direct dependency table above, we can iteratively construct the extended dependency tree:\n\n\nCode\nfind_all_deps <- function(upstream, data){\n  print(upstream)\n  dt <- tibble()\n  dt2 <- data\n  i <- 1\n  while(nrow(dt2) > nrow(dt)){\n    print(i)\n    dt <- dt2\n    n <- paste0(\"upstream\", i) \n    dt2 <- dt %>% \n      rename(upstream = downstream) %>% \n      left_join(dep_lookup_tbl %>% select(-type), by = \"upstream\") %>% \n      rename(!!quo_name(n) := upstream)\n    i <- i + 1\n  }\n  \n  dep <- dt2 %>%\n    pivot_longer(\n      cols = c(contains(\"upstream\"),  \"downstream\"),\n      names_to = \"dump\", values_to = \"downstream\") %>%\n    distinct(downstream) %>%\n    filter(!is.na(downstream)) %>%\n    mutate(downstream = sort(downstream))\n  return(dep)\n}\n\ndep_all <- dep_lookup_tbl %>% \n  arrange(-desc(upstream)) %>% \n  nest(direct_deps = -upstream) %>% \n  mutate(all_deps = map2(upstream, direct_deps, find_all_deps))\n\n(edges <- dep_all %>% \n    select(-direct_deps) %>% \n    unnest(all_deps) %>% \n    filter(!is.na(upstream), !is.na(downstream)))\n\n\n\n\n\n\n\n\n\n\n# A tibble: 551,713 × 2\n   upstream downstream\n   <chr>    <chr>     \n 1 a4Core   nlcv      \n 2 abc      abctools  \n 3 abc      EasyABC   \n 4 abc      ecolottery\n 5 abc      nlrx      \n 6 abc      paleopop  \n 7 abc      poems     \n 8 abc.data abc       \n 9 abc.data abctools  \n10 abc.data EasyABC   \n# … with 551,703 more rows\n\n\nThe plot below shows the number of dependencies and reverse dependencies a package has.\n\n\nCode\nnodes <- tibble(id = unique(c(edges$upstream, edges$downstream))) %>% \n  left_join(edges %>% count(upstream, name = \"n_revdep\"), by = c(\"id\" = \"upstream\")) %>% \n  left_join(edges %>% count(downstream, name = \"n_dep\"), by = c(\"id\" = \"downstream\")) %>% \n  filter(!is.na(id)) %>% \n  mutate(n_revdep = ifelse(is.na(n_revdep), 0, n_revdep),\n         n_dep = ifelse(is.na(n_dep), 0, n_dep))\n\n################################################################\n# deriving color categories\nrecommended <- raw %>% filter(Priority == \"recommended\") %>% pull(Package)\n\nbase <- c(\"base\", \"compiler\", \"datasets\", \"grDevices\", \"graphics\", \"grid\", \"methods\", \"parallel\", \"splines\", \"stats\", \"stats4\", \"tcltk\", \"tools\", \"translations\", \"utils\")\n\nr_lib_gh <- gh(\"GET /orgs/{username}/repos\", username = \"r-lib\", .limit = 200)\nr_lib <- vapply(r_lib_gh, \"[[\", \"\", \"name\")\n\nr_tidyverse_gh <- gh(\"GET /orgs/{username}/repos\", username = \"tidyverse\", .limit = 40)\ntidyverse <- vapply(r_tidyverse_gh, \"[[\", \"\", \"name\")\n\nnodes <- nodes %>% \n  mutate(category = \n           case_when(id %in% tidyverse ~ \"tidyverse\", \n                     id %in% base ~ \"base\",\n                     id %in% r_lib ~ \"r-lib\",\n                     id %in% recommended ~ \"recommended\",\n                     TRUE ~ \"zzz\"))\n################################################################\n# to deal with zero mark after sqrt tranform\n# https://github.com/tidyverse/ggplot2/issues/980\nmysqrt_trans <- function() {\n    scales::trans_new(\"mysqrt\", \n              transform = base::sqrt,\n              inverse = function(x) ifelse(x<0, 0, x^2),\n              domain = c(0, Inf))\n}\n\np <- nodes %>% \n  mutate(tooltip = glue::glue(\"Pkg: {id}, dep: {n_dep}, revdep: {n_revdep}\")) %>% \n  ggplot(aes(x = n_dep, y = n_revdep)) + \n  geom_point_interactive(aes(tooltip = tooltip)) +\n  ggrepel::geom_text_repel(\n    data = nodes %>% filter(n_revdep > 3100),\n    aes(color= category, label = id), min.segment.length = 0) +\n  scale_color_brewer(palette = \"Set1\") + \n  scale_y_continuous(breaks = c(0,  50, 200, 500, 1000, 2500, 5000, 7500, 10000, 15000), trans = \"mysqrt\") + \n  scale_x_continuous(breaks = c(0, 1, 5, 10, 20, 40, 80, 120, 160, 200), trans = \"mysqrt\") + \n  theme(panel.grid.minor = element_blank(),\n        legend.position = \"bottom\") + \n  xlab(\"Number of dependencies\") + \n  ylab(\"Number of reverse dependencies\")\n\ngirafe(ggobj = p, width_svg = 16, height_svg = 12)\n\n\n\n\n\n\n\nThe x and y-axes show the number of dependencies and reverse dependencies of a package. Both coordinates are square root transformed to accommodate for the skewness in both measures. Packages with more than 3100 reverse dependencies are labelled. The label color denotes four groups: those in base R, those labelled as “recommended” by CRAN, and those listed in the tidyverse and r-lib organisations on GitHub. Expand the color group below to view the package membership:\n\n\ncolor group\n\n\n\n\n\n\n\n  \n  \n    \n      category\n      packages\n    \n  \n  \n    base\nbase, compiler, datasets, graphics, grDevices, grid, methods, parallel, splines, stats, stats4, tcltk, tools, utils\n    tidyverse\nblob, dbplyr, dplyr, dtplyr, forcats, ggplot2, glue, googledrive, googlesheets4, haven, hms, lubridate, magrittr, modelr, multidplyr, nycflights13, purrr, readr, readxl, reprex, rvest, stringr, tibble, tidyr, tidyverse, vroom\n    r-lib\narchive, askpass, available, backports, bench, brio, cachem, callr, carrier, cli, clisymbols, clock, commonmark, conflicted, coro, covr, cpp11, crayon, credentials, debugme, desc, devtools, downlit, ellipsis, err, evaluate, fastmap, filelock, fs, gargle, generics, gert, gh, gitcreds, gmailr, gtable, here, httr, httr2, jose, keyring, later, lifecycle, lintr, liteq, lobstr, memoise, mockery, pak, pillar, pingr, pkgbuild, pkgcache, pkgconfig, pkgdepends, pkgdown, pkgload, prettycode, prettyunits, processx, progress, ps, R6, ragg, rappdirs, rcmdcheck, rematch2, remotes, rex, rlang, roxygen2, rprojroot, scales, sessioninfo, showimage, sodium, styler, svglite, systemfonts, testthat, textshaping, tidyselect, tzdb, urlchecker, usethis, vctrs, waldo, whoami, withr, xml2, xmlparsedata, xopen, ymlthis, zeallot, zip, roxygen2md, diffviewer, vdiffr, asciicast, cliapp, decor, meltr, sloop, tracer, io, conf, webfakes\n    recommended\nboot, class, cluster, codetools, foreign, KernSmooth, lattice, MASS, Matrix, mgcv, nlme, nnet, rpart, spatial, survival\n  \n  \n  \n\n\n\n\n\nThe plot is interactive so you can hover over points of your interest to read the package name and its numbers of (reverse) dependency.\n\n\nIt’s okay to be a couch Pareto\nAs you would have already noticed, the distribution of the number of reverse dependencies is highly skewed, even after the square root transformation. To better visualise how the lower number of reverse dependency is distributed, we can plot its cumulative distribution:\n\n\nCode\nprct_tbl <- purrr::map_dfr(\n  unique(nodes$n_revdep) %>% sort(), \n  ~tibble(n = .x, prct = nrow(filter(nodes, n_revdep <= .x)) /nrow(nodes)))\n\ntgt_pnts <- prct_tbl %>% \n  mutate(p = round(prct, digits = 3)) %>% \n  filter(p %in% c(0.9, 0.95, 0.99, 0.995, 0.999) | prct == 1 |n %in% c(0, 1, 5)) %>% \n  group_by(p) %>% \n  filter(n == min(n))\n\nprct_tbl %>% \n  ggplot() +\n  geom_line(aes(x = n, y = prct)) + \n  geom_point(data = tgt_pnts, aes(x = n, y = prct)) + \n  geom_label(data = tgt_pnts, aes(x = n, y = prct + 0.01, label = p)) + \n  scale_x_continuous(breaks = round(c(tgt_pnts$n, max(prct_tbl$n)))) + \n  coord_trans(\"pseudo_log\") + \n  theme(panel.grid.minor = element_blank()) + \n  ylab(\"Percentage of CRAN pkgs with <= n reverse dependencies\")\n\n\n\n\n\n\n\n\n\nWhether you have guessed or not:\n\n71.7% of CRAN packages don’t have any reverse dependency;\nfewer than 10% of the packages on CRAN have more than 5 reverse dependencies; and\nonly 1% of the packages have more than 415 reverse dependencies\n\nSo while the majority of the R packages do not need reverse dependency checks, a small number of core packages need to test against hundreds or even thousands of reverse dependencies for every new release.\nAlternatively, we can rank the packages by their number reverse dependencies (the package with the largest number of reverse dependencies is ranked first). The advantage of this is that there turns out to be a distribution that can capture the shape well: the Zipf–Mandelbrot distribution, the generalised zipf distribution, which is commonly used to model corpus frequency in linguistics:\n\n\nCode\nnodes_rank <- nodes %>%  \n  mutate(rank = rank(-n_revdep, ties.method = \"first\")) %>% \n  filter(n_revdep > 0)\n\ndt_pos <- nodes %>% filter(n_revdep > 0) %>% pull(n_revdep) %>% sort(decreasing = TRUE)\npred1 <- fitrad(dt_pos, \"mand\") %>% radpred()\nfitted <- tibble(\n  rank = pred1$rank,\n  mand = pred1$abund, \n  count = dt_pos)\n\np2 <- nodes_rank %>% \n  ggplot(aes(x = rank, y = n_revdep)) + \n  geom_point_interactive(aes(tooltip = id)) + \n  geom_line(data = fitted, aes(x = rank, y = mand), color = \"#314f40\") + \n  ggrepel::geom_text_repel(\n    data = nodes_rank %>% filter(n_revdep > 3100), \n    aes(label = id, color = category),\n    min.segment.length = 0) + \n  scale_y_continuous(breaks = c(10, 50, 200, 500, 1000, 2500, 5000, 7500, 10000, 15000)) + \n  scale_x_continuous(breaks = c(0, 1, 10, 50, 100, 200, 500, 1000, 2000, 5000, 10000, 20000)) + \n  coord_trans(x = \"log\", y = \"sqrt\") + \n  scale_color_brewer(palette = \"Set1\") + \n  theme_bw() + \n  theme(panel.grid.minor = element_blank(), \n        legend.position = \"bottom\") + \n  ylab(\"number of reverse dependencies\")\ngirafe(ggobj = p2, width_svg = 16, height_svg = 12)\n\n\n\n\n\n\n\n\n\nContender for the next isoband\nWe can also find packages with similar characteristics as isoband: those with a huge number of reverse dependencies (n > 3000) while not managed by base R or RStudio or listed as recommended on CRAN:\n\n\n\n\n\n\n  \n  \n    \n      Package\n      # of reverse dependency\n    \n  \n  \n    Rcpp\n8401\n    fansi\n6777\n    utf8\n6777\n    digest\n6418\n    colorspace\n5041\n    RColorBrewer\n5030\n    viridisLite\n4895\n    farver\n4875\n    labeling\n4865\n    munsell\n4864\n    stringi\n4573\n    jsonlite\n4326\n    yaml\n3031\n    mime\n3020\n  \n  \n  \n\n\n\n\nIn a quick glance, these packages fall into three categories:\n\nColor packages: colorspace, RColorBrewer, viridisLite, munsell\nSuper giants:\n\n\nRcpp for interfacing to C++ for fast computation;\njsonlite for parsing JSON; and\nstringi for string manipulation.\n\n\nHidden dependencies: These packages don’t have a huge number of direct dependencies themselves but is imported by a giant package:\n\n\nimported by ggplot2:\n\ndigest for hashing object in R, and\nfarver and labelling are imported by scales, which in turn is imported by ggplot2\n\nimported by tibble:\n\nfansi for ANSI text formatting, and\nutf8 for processing UTF encoding, imported by pillar, which is imported by tibble\n\nimported by shiny: mime for converting file name extension, and\nimported by knitr: yaml for YAML text conversion\n\n\n\nThe social network\nFinally, the network diagram! I have tried to include all 17,354 nodes and 551,713 pairs of edges in a single diagram and I do not blame my computer for rejecting it with\nvector memory exhausted (limit reached?). \nTo avert this error, I plot a subset of the large number of packages with n ≤ 5 reverse dependencies. Specifically, for each n ≤ 5, I randomly select 40 packages with the given n. After playing around with layouts and other aesthetics, here is the result…\n\n\ncode\n\n\n\nCode\nwirdos <- c(\"brglm\", \"profileModel\", \"ExPosition\", \"prettyGraphs\", \"seasonal\", \"x13binary\", \"scalreg\", \"lars\", \"elasticnet\")\nmore_than5 <- nodes %>% filter(n_revdep > 5) %>% filter(!id %in% wirdos)\nset.seed(123)\nless_than5 <- nodes %>% \n  filter(n_revdep <= 5) %>% \n  nest_by(n_revdep, .key = \"nested\") %>% \n  mutate(id = list(map(nested, ~sample(.x, size = 40))$id)) %>% \n  unnest(id) %>% \n  select(-nested) %>% \n  ungroup()\n\nnew_nodes <- bind_rows(more_than5, less_than5)\nnew_edges <- dep_lookup_tbl %>% filter(upstream %in% new_nodes$id & downstream %in% new_nodes$id) %>% select(-type)\nnew_nodes <- new_nodes %>% filter(id %in% c(new_edges$downstream, new_edges$upstream))\ng <- tbl_graph(nodes = new_nodes, edges = new_edges, directed = TRUE)\n\nggraph(g, layout = \"fr\") + \n  geom_edge_link(alpha = 0.1) + \n  geom_node_label(data = ~ .x %>% filter(n_revdep >= 3200), aes(label =id), repel = TRUE) + \n  geom_node_point(aes(size = n_revdep), alpha = 0.5) + \n  theme_void() + \n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFootnotes\n\n\nOn 5th Oct, CRAN sent out a massive email to inform 4747 downstream package maintainers of the potential archive of package isoband on 2022-10-19.↩︎"
  },
  {
    "objectID": "projects/ferrn/ferrn.html",
    "href": "projects/ferrn/ferrn.html",
    "title": "Visual Diagnostics for Constrained Optimisation with Application to Guided Tours",
    "section": "",
    "text": "A guided tour helps to visualise high-dimensional data by showing low-dimensional projec tions along a projection pursuit optimisation path. Projection pursuit is a generalisation of principal component analysis in the sense that different indexes are used to define the interestingness of the projected data. While much work has been done in developing new indexes in the literature, less has been done on understanding the optimisation. Index functions can be noisy, might have multiple local maxima as well as an optimal maximum, and are constrained to generate orthonormal projection frames, which complicates the optimization. In addition, projection pursuit is primarily used for exploratory data analysis, and finding the local maxima is also useful. The guided tour is especially useful for exploration because it conducts geodesic interpolation connecting steps in the optimisation and shows how the projected data changes as a maxima is approached. This work provides new visual diagnostics for examining a choice of optimisation procedure based on the provision of a new data object which collects information throughout the optimisation. It has helped to diagnose and fix several problems with projection pursuit guided tour. This work might be useful more broadly for diagnosing optimisers and comparing their performance. The diagnostics are implemented in the R package ferrn."
  },
  {
    "objectID": "projects/cubble/cubble.html",
    "href": "projects/cubble/cubble.html",
    "title": "cubble: An R Package for Organizing and Wrangling Multivariate Spatio-temporal Data",
    "section": "",
    "text": "Multivariate spatio-temporal data refers to multiple measurements taken across space and time. For many analyses, spatial and time components can be separately studied: for example, to explore the temporal trend of one variable for a single spatial location, or to model the spatial distribution of one variable at a given time. However for some studies, it is important to analyse different aspects of the spatio-temporal data simultaneouly, like for instance, temporal trends of multiple variables across locations. In order to facilitate the study of different portions or combinations of spatio-temporal data, we introduce a new data structure, cubble, with a suite of functions enabling easy slicing and dicing on the different components spatio-temporal components. The proposed cubble structure ensures that all the components of the data are easy to access and manipulate while providing flexibility for data analysis. In addition, cubble facilitates visual and numerical explorations of the data while easing data wrangling and modelling. The cubble structure and the functions provided in the cubble R package equip users with the capability to handle hierarchical spatial and temporal structures. The cubble structure and the tools implemented in the package are illustrated with different examples of Australian climate data."
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "H. Sherry Zhang",
    "section": "",
    "text": "Jun 1, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov 20, 2021\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "H. Sherry Zhang",
    "section": "",
    "text": "I’m Sherry, a final year PhD student at Monash University supervised by Di Cook, Patricia Menéndez, Ursula Laa, and Nicolas Langrené. My research provides data wrangling and visualisation tools for analysing multivariate spatio-temporal data. One of my projects, ferrn, focus on creating diagnostic plots to understand the optimisers in projection pursuit, a multivariate dimension reduction method. I have also been working on a spatio-temporal data class, cubble (cubical-tibble), to help with data wrangling and making glyph maps."
  },
  {
    "objectID": "cv/H.Sherry-Zhang.html",
    "href": "cv/H.Sherry-Zhang.html",
    "title": "H. Sherry Zhang",
    "section": "",
    "text": "Publications\n\n\n\n/var/folders/_t/v9kjp3yn2k73wm16y_jlphgsbbd6_6/T//Rtmps1Z0fl/file146703f7a3af5.yaml\n\n\n\n\n\nTalks\n\n\n# A tibble: 6 × 3\n  what                                                               when  with \n  <glue>                                                             <chr> <chr>\n1 Switching between space and time: Spatio-temporal analysis with c… \"Feb… \"The…\n2 Switching between space and time: Spatio-temporal analysis with c… \"Nov… \"ECS…\n3 Switching between space and time: Spatio-temporal analysis with c… \"Nov… \"CAN…\n4 Cubble: An R package for organizing and wrangling multivariate sp… \"Jun… \"Use…\n5 Visual diagnostics for constrained optimisation with application … \"Jul… \"Use…\n6 Exploration of judicial facial expression in videos of legal proc… \"Oct… \"You…\n\n\n\n\nTeaching\n\n\n\n# A tibble: 3 × 3\n  what                                                               when  with \n  <glue>                                                             <chr> <chr>\n1 \\strong{ETC5512 Wild Caught Data}\\space \\url{https://wcd.numbat.s… \"Sem… \"Mon…\n2 \\strong{ETC5521 Exploratory Data Analysis}\\space \\url{https://eda… \"Sem… \"Mon…\n3 \\strong{ETC1010/ETC5510 Introduction to Data Analysis}\\space \\url… \"Sem… \"Mon…\n\n\n\n\nWorkshop\n\n\n# A tibble: 2 × 3\n  what                                                               when  with \n  <glue>                                                             <chr> <chr>\n1 \\strong{Wrangling spatio-temporal data with R}\\space\\url{https://… \"Dec… \"WOM…\n2 \\strong{Switching between space and time: Spatio-temporal analysi… \"Sep… \"R L…\n\n\n\n\nSoftware\n\n\n\n# A tibble: 5 × 3\n  what                                                               when  with \n  <glue>                                                             <chr> <chr>\n1 \\textbf{cubble}: a multivariate data structure for organizing and… \"202… <NA> \n2 \\textbf{rjtools}: tools for authors to write, check, and submit a… \"202… <NA> \n3 \\textbf{rj}: tools to help the (associate) editors with the revie… \"202… <NA> \n4 \\textbf{ferrn}: a package for visual diagnostics of projection pu… \"202… <NA> \n5 \\textbf{tourr}: implementing a data structure for collecting guid… \"202… <NA> \n\n\n\n\nAwards\n\n\n# A tibble: 5 × 3\n  what                                                              when   with \n  <glue>                                                            <chr>  <chr>\n1 People's Choice Award for the presentation in ECSS Miniconference \"2022\" Stat…\n2 Postgraduate (PhD) Research Scholarship                           \"20 -… CSIR…\n3 Co-Funded Graduate Research Scholarship                           \" 20 … Mona…\n4 Dean's Honours List                                               \"2020\" Mona…\n5 Summer Vacation Research Scholarship                              \"2019\" Mona…\n\n\n\n\nAffiliation\n\n\n# A tibble: 4 × 3\n  what                                                               when  with \n  <chr>                                                              <chr> <chr>\n1 Non-Uniform Monash Business Analytics Team (Monash NUMBATs)        \"1. \" <NA> \n2 Statistical Society of Australia (VIC branch)                      \"2. \" <NA> \n3 Australian Research Council Centre of Excellence in Mathematical … \"3. \" <NA> \n4 Commonwealth Scientific and Industrial Research Organisation (CSI… \"4. \" <NA> \n\n\n\n\nService\n\n\n# A tibble: 1 × 3\n  what                                                           when with \n  <glue>                                                        <dbl> <chr>\n1 Monash NUMBATs seminar organiser: \\url{https://numbat.space/}  2022 <NA>"
  }
]